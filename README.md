
# Crawlers-Block

# C'est quoi au juste?
Les termes de crawler, robot de crawl ou spider, désignent dans le monde de l'informatique un robot d'indexation. Concrètement, il s'agit d'un logiciel qui a pour principale mission d'explorer le Web afin d'analyser le contenu des documents visités et les stocker de manière organisée dans un index. 

# Fonctionnement
Les crawlers peuvent être programmés pour parcourir le Web avec des objectifs déterminés. Ils sont actifs en permanence et visitent les pages selon les instructions qui leur sont données.

# Pour le script
Veuillez consulter ce lien:

<a href="https://github.com/Onja74/Crawlers_Block-TP2/blob/main/CrawlersBlock.sh"> script crawlers_block </a>
